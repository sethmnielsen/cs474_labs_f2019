{"cells":[{"source":["from IPython import get_ipython\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" <a\n"," href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab9.ipynb\"\n","   target=\"_parent\">\n","   <img\n","     src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n","     alt=\"Open In Colab\"/>\n"," </a>"],"metadata":{}},{"cell_type":"markdown","source":[" # Lab 9: Deep Reinforcement Learning\n","\n"," ## Objective\n","\n"," - Build DQN and PPO Deep RL algorithms\n"," - Learn the difference between Q Learning and Policy Gradient techniques\n","\n"," ## Deliverable\n","\n"," For this lab you will submit an ipython notebook via learning suite. This lab gives you a lot of code, and you should only need to modify two of the cells of this notebook. Feel free to download and modify this notebook or create your own. The below code is given for your convinience. You can modify any of the given code if you wish.\n","\n"," ## Tips\n","\n"," Deep reinforcement learning is difficult. We provide hyperparameters, visualizations, and code for gathering experience, but require you to code up algorithms for training your networks.\n","\n"," - Your networks should be able to demonstrate learning on cartpole within a minute of wall time.\n","\n"," - Understand what your the starter code is doing. This will help you with the *TODO* sections. The main code block is similar for the two algorithms with some small yet important differences.\n","\n"," - We provide hyperparameters for you to start with. Feel free to experiment with different values, but these worked for us.\n","\n"," - **Print dtypes and shapes** throughout your code to make sure your tensors look the way you expect.\n","\n"," - The DQN algorithm is significantly more unstable than PPO. Even with a correct implementation it may fail to learn every 1/10 times.\n","\n"," - Unfortunately visualizing your agent acting in the environment is non-trivial in Colab. You can visualize your agent by running this code locally and uncommenting the `env.render()` line.\n","\n"," ## Grading\n","\n"," - 35% Part 1: DQN *TODO* methods\n"," - 35% Part 2: PPO *TODO* methods\n"," - 20% Part 3: Cartpole learning curves\n"," - 10% Tidy legible code\n","\n"," ___\n","\n"," ## Part 1\n","\n"," ### DQN\n","\n"," Deep Q-Network (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a Q-learning algorithm that learns values for state-action pairs.\n","\n"," Actions are sampled according to an $\\epsilon-greedy$ policy to help with exploration of the state space. Every time an action is sampled, the agent chooses a random action with $\\epsilon$ probability. Otherwise, the agent selects the action with the highest Q-value for a state. $\\epsilon$ decays over time according to $\\epsilon \\gets \\epsilon * epsilon\\_decay$.\n","\n"," Tuples of state, action, reward, next_state, and terminal $(s,a,r,s',d)$ are collected during training. Every $learn\\_frequency$ steps $sample\\_size$ tuples are sampled and made into 5 tensors tensors of states, actions, rewarads, next_states, and terminals.\n","\n"," The loss for a batch of size N is given below.\n","\n"," $Loss=\\frac{1}{N}\\sum \\bigg(Q(s,a) - (r + \\gamma \\underset{a'\\sim A}{max} \\hat{Q}(s',a')(1-d))\\bigg)^2 $\n","\n"," Loss is calculated and used to update the Q-Network. The target network $\\hat{Q}$ begins as a copy of the Q network but is not updated by the optimizer. Every $target\\_update$ steps, the target network is updated with the parameters of the Q-Network. This processes is a type of bootstrapping.\n","\n"," ### TODO\n","\n"," - Implement get action method with e-greedy policy\n"," - Implement sample batch method\n"," - Implement DQN learning algorithm\n","\n"," ## Part 2\n","\n"," ### PPO\n","\n"," Proximal Policy Optimization (https://arxiv.org/pdf/1707.06347.pdf) is a type of policy gradient method. Instead of calculating Q-values, we train a network $\\pi$ to optimize the probability of taking good actions directly, using states as inputs and actions as outputs. PPO also uses a value network $V$ that estimates state values in order to estimate the advantage $\\hat{A}$.\n","\n"," Tuples of state, action distribution, action taken, and return $(s,\\pi(s), a,\\hat{R})$ are gathered for several rollouts. After training on this experience, these tuples are discarded and new experience is gathered.\n","\n"," Loss for the value network and the policy network are calculated according to the following formula:\n","\n"," $Loss=ValueLoss+PolicyLoss$\n","\n"," $ValueLoss=\\frac{1}{N}\\sum \\bigg(\\hat{R} - V(s) \\bigg)^2 $\n","\n"," $PolicyLoss=-\\frac{1}{N}\\sum \\min\\bigg( \\frac{\\pi'(a|s)}{\\pi(a|s)} \\hat{A}, clip(\\frac{\\pi'(a|s)}{\\pi(a|s)},1-\\epsilon,1+\\epsilon) \\hat{A} \\bigg) $\n","\n"," $\\hat{R}_t = \\sum_{i=t}^H \\gamma^{i-1}r_i$\n","\n"," $\\hat{A}_t=\\hat{R}_t-V(s_t)$\n","\n"," Here, $\\pi'(a|s)$ is the probability of taking an action given a state under the current policy and $\\pi(a|s)$ is the probability of taking an action given a state under the policy used to gather data. In the loss function, $a$ is the action your agent actually took and is sampled from memory.\n","\n"," Additionally, the $clip$ function clips the value of the first argument according to the lower and upper bounds in the second and third arguments resectively.\n","\n"," Another important note: Your the calculation of your advantage $\\hat{A}$ should not permit gradient flow from your policy loss calculation. In other words, make sure to call `.detach()` on your advantage.\n","\n"," ### TODO\n","\n"," - Implement calculate return method\n"," - Implement get action method\n"," - Implement PPO learning algorithm\n","\n"," ## Part 3\n","\n"," ### Cartpole\n","\n"," Cartpole is a simple environment to get your agent up and running. It has a continuous state space of 4 dimensions and a discrete action space of 2. The agent is given a reward of 1 for each timestep it remains standing. Your agent should be able to reach close to 200 cumulative reward for an episode after a minute or two of training. The below graphs show example results for dqn (left) and ppo (right).\n","\n"," ![alt text](https://drive.google.com/uc?export=view&id=1Bpz1jOPMF1zJMW6XBJJ44sJ-RmO_q6_U)\n"," ![alt text](https://drive.google.com/uc?export=view&id=1M1yygXhLKDL8qfRXn7fh_K-zq7-pQRhY)\n","\n"," ### TODO\n","\n"," - Train DQN and PPO on cartpole\n"," - Display learning curves with average episodic reward per epoch"],"metadata":{}},{"cell_type":"markdown","source":[" # Starter Code"],"metadata":{}},{"cell_type":"markdown","source":[" ## Init"],"metadata":{}},{"source":["# get_ipython().system(' pip3 install gym')\n","# get_ipython().system(' pip3 install torch')\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["import gym\n","import torch\n","import torch.nn as nn\n","from itertools import chain\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from IPython.core.debugger import Pdb\n","pdb = Pdb()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## DQN"],"metadata":{}},{"cell_type":"markdown","source":[" ### TODO"],"metadata":{}},{"source":["\n","\n","def get_action_dqn(network, state, epsilon, epsilon_decay):\n","    \"\"\"Select action according to e-greedy policy and decay epsilon\n","\n","        Args:\n","            network (QNetwork): Q-Network\n","            state (np-array): current state, size (state_size)\n","            epsilon (float): probability of choosing a random action\n","            epsilon_decay (float): amount by which to decay epsilon\n","\n","        Returns:\n","            action (int): chosen action [0, action_size)\n","            epsilon (float): decayed epsilon\n","    \"\"\"\n","    new_epsilon = epsilon * epsilon_decay\n","    r = np.random.rand()\n","    if r < epsilon:\n","        action = np.random.randint(2)\n","    else:\n","        state_t = torch.cuda.FloatTensor(state)\n","        action = torch.argmax(network(state_t)).item()\n","\n","    return action, new_epsilon\n","\n","\n","def prepare_batch(memory, batch_size):\n","    \"\"\"Randomly sample batch from memory\n","        Prepare cuda tensors\n","\n","        Args:\n","            memory (list): state, action, next_state, reward, done tuples\n","            batch_size (int): amount of memory to sample into a batch\n","\n","        Returns:\n","            state (tensor): float cuda tensor of size (batch_size x state_size()\n","            action (tensor): long tensor of size (batch_size)\n","            next_state (tensor): float cuda tensor of size (batch_size x state_size)\n","            reward (tensor): float cuda tensor of size (batch_size)\n","            done (tensor): float cuda tensor of size (batch_size)\n","    \"\"\"\n","    # idx = np.random.randint(0, len(memory), batch_size)\n","    # sample = [memory[i] for i in idx]\n","    # [(s.append(s_), a.append(a_), n.append(n_), r.append(r_), \n","    #                                     d.append(d_)) for s_, a_, n_, r_, d_ in sample]\n","\n","    sample = random.sample(memory, batch_size)\n","    s, a, n, r, d = [], [], [], [], []\n","    for i in range(batch_size):\n","        s.append( sample[i][0] )\n","        a.append( sample[i][1] )\n","        n.append( sample[i][2] )\n","        r.append( sample[i][3] )\n","        d.append( sample[i][4] )\n","\n","    state      = torch.cuda.FloatTensor(s)\n","    action     = torch.cuda.LongTensor (a)\n","    next_state = torch.cuda.FloatTensor(n)\n","    reward     = torch.cuda.FloatTensor(r)\n","    done       = torch.cuda.FloatTensor(d)\n","\n","    return state, action, next_state, reward, done\n","\n","\n","def learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update):\n","    \"\"\"Update Q-Network according to DQN Loss function\n","        Update Target Network every target_update global steps\n","\n","        Args:\n","            batch (tuple): tuple of state, action, next_state, reward, and done tensors\n","            optim (Adam): Q-Network optimizer\n","            q_network (QNetwork): Q-Network\n","            target_network (QNetwork): Target Q-Network\n","            gamma (float): discount factor\n","            global_step (int): total steps taken in environment\n","            target_update (int): frequency of target network update\n","    \"\"\"\n","    optim.zero_grad()\n","    state, action, next_state, reward, done = batch\n","    a = action[:,None]    \n","    \n","    q_val = q_network(state).gather(1, a).squeeze()\n","    reward_dis = reward + gamma * torch.max(target_network(next_state), dim=1)[0]\n","    terminal = 1 - done\n","    loss = torch.mean( (q_val - reward_dis*terminal)**2 )\n","\n","    loss.backward( )\n","    optim.step()\n","    \n","    if global_step % target_update == 0:\n","        target_network.load_state_dict( q_network.state_dict() )\n","\n","    \n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ###Modules"],"metadata":{}},{"source":["# Q-Value Network\n","class QNetwork(nn.Module):\n","  def __init__(self, state_size, action_size):\n","    super().__init__()\n","    hidden_size = 8\n","\n","    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n","                             nn.ReLU(),\n","                             nn.Linear(hidden_size, hidden_size),\n","                             nn.ReLU(),\n","                             nn.Linear(hidden_size, hidden_size),\n","                             nn.ReLU(),\n","                             nn.Linear(hidden_size, action_size))\n","\n","  def forward(self, x):\n","    \"\"\"Estimate q-values given state\n","\n","      Args:\n","          state (tensor): current state, size (batch x state_size)\n","\n","      Returns:\n","          q-values (tensor): estimated q-values, size (batch x action_size)\n","    \"\"\"\n","    return self.net(x)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### Main\n"," -----------------------------------------------------------------------------------------------------#\n"," ---------------------------------------- BEGIN DQN MAIN ---------------------------------------------#\n"," -----------------------------------------------------------------------------------------------------#"],"metadata":{}},{"source":["def dqn_main():\n","  # Hyper parameters\n","  lr = 1e-3\n","  epochs = 500\n","  start_training = 1000\n","  gamma = 0.99\n","  batch_size = 32\n","  epsilon = 1\n","  epsilon_decay = .9999\n","  target_update = 1000\n","  learn_frequency = 2\n","\n","  # Init environment\n","  state_size = 4\n","  action_size = 2\n","  env = gym.make('CartPole-v1', )\n","\n","  # Init networks\n","  q_network = QNetwork(state_size, action_size).cuda()\n","  target_network = QNetwork(state_size, action_size).cuda()\n","  target_network.load_state_dict(q_network.state_dict())\n","\n","  # Init optimizer\n","  optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n","\n","  # Init replay buffer\n","  memory = []\n","\n","  # Begin main loop\n","  results_dqn = []\n","  global_step = 0\n","  loop = tqdm(total=epochs, position=0, leave=False)\n","  for epoch in range(epochs):\n","\n","    # Reset environment\n","    state = env.reset()\n","    done = False\n","    cum_reward = 0  # Track cumulative reward per episode\n","\n","    # Begin episode\n","    while not done and cum_reward < 200:  # End after 200 steps\n","      # Select e-greedy action\n","      action, epsilon = get_action_dqn(\n","          q_network, state, epsilon, epsilon_decay)\n","\n","      # Take step\n","      next_state, reward, done, _ = env.step(action)\n","      # env.render()\n","\n","      # Store step in replay buffer\n","      memory.append((state, action, next_state, reward, done))\n","\n","      cum_reward += reward\n","      global_step += 1  # Increment total steps\n","      state = next_state  # Set current state\n","\n","      # If time to train\n","      if global_step > start_training and global_step % learn_frequency == 0:\n","\n","        # Sample batch\n","        batch = prepare_batch(memory, batch_size)\n","\n","        # Train\n","        learn_dqn(batch, optim, q_network, target_network,\n","                  gamma, global_step, target_update)\n","\n","    # Print results at end of episode\n","    results_dqn.append(cum_reward)\n","    loop.update(1)\n","    loop.set_description('Episodes: {} Reward: {}'.format(epoch, cum_reward))\n","\n","  return results_dqn\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["results_dqn = dqn_main()\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plt.plot(results_dqn)\n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## PPO"],"metadata":{}},{"cell_type":"markdown","source":[" ### TODO"],"metadata":{}},{"source":["\n","\n","def calculate_return(memory, rollout, gamma):\n","    \"\"\"Return memory with calculated return in experience tuple\n","\n","        Args:\n","            memory (list): (state, action, action_dist, return) tuples\n","            rollout (list): (state, action, action_dist, reward) tuples from last rollout\n","            gamma (float): discount factor\n","\n","        Returns:\n","            list: memory updated with (state, action, action_dist, return) tuples from rollout\n","    \"\"\"\n","    ret = 0\n","    for s, a, a_dist, r in reversed(rollout):\n","        ret = r + ret*gamma\n","        memory.append((s, a, a_dist, ret))\n","\n","    return memory\n","\n","\n","def get_action_ppo(network, state):\n","  \"\"\"Sample action from the distribution obtained from the policy network\n","\n","    Args:\n","        network (PolicyNetwork): Policy Network\n","        state (np-array): current state, size (state_size)\n","\n","    Returns:\n","        int: action sampled from output distribution of policy network\n","        array: output distribution of policy network\n","  \"\"\"\n","  s = torch.cuda.FloatTensor(state).unsqueeze(0)\n","  a_dist = network(s)\n","  a = torch.multinomial(a_dist, 1).item()\n","  return a, a_dist.detach()\n","\n","\n","def learn_ppo(optim, policy, value, memory_dataloader, epsilon, policy_epochs):\n","    \"\"\"Implement PPO policy and value network updates. Iterate over your entire \n","        memory the number of times indicated by policy_epochs.    \n","\n","        Args:\n","            optim (Adam): value and policy optimizer\n","            policy (PolicyNetwork): Policy Network\n","            value (ValueNetwork): Value Network\n","            memory_dataloader (DataLoader): dataloader with (state, action, action_dist, return, discounted_sum_rew) tensors\n","            epsilon (float): trust region\n","            policy_epochs (int): number of times to iterate over all memory\n","    \"\"\"\n","    mse = nn.MSELoss()\n","    for _ in range(policy_epochs):\n","        for s, a, a_d, r in memory_dataloader:\n","            optim.zero_grad()\n","\n","            state = s.type_as(torch.cuda.FloatTensor())\n","            action = a.type_as(torch.cuda.LongTensor())\n","            action_dist = a_d.type_as(torch.cuda.FloatTensor()).squeeze()\n","            ret = r.type_as(torch.cuda.FloatTensor())\n","            \n","            # value loss\n","            v_s = value(state).squeeze()\n","            v_loss = mse(ret, v_s)\n","\n","            # policy loss\n","            adv = (ret - v_s).detach()\n","            past_prob = action_dist.gather(1,action[:,None]).squeeze()\n","            cur_prob  = policy(state).gather(1, action[:,None]).squeeze()\n","            ratio = cur_prob / past_prob\n","            left = ratio * adv\n","            right = torch.clamp(ratio, 1-epsilon, 1+epsilon) * adv\n","            p_loss = torch.min(left, right)\n","            p_loss = -torch.mean(p_loss)\n","\n","            loss = p_loss + v_loss\n","\n","            loss.backward()\n","            optim.step()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ###Modules"],"metadata":{}},{"source":["# Dataset that wraps memory for a dataloader\n","\n","\n","class RLDataset(Dataset):\n","  def __init__(self, data):\n","    super().__init__()\n","    self.data = []\n","    for d in data:\n","      self.data.append(d)\n","\n","  def __getitem__(self, index):\n","    return self.data[index]\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","\n","# Policy Network\n","class PolicyNetwork(nn.Module):\n","  def __init__(self, state_size, action_size):\n","    super().__init__()\n","    hidden_size = 8\n","\n","    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n","                             nn.ReLU(),\n","                             nn.Linear(hidden_size, hidden_size),\n","                             nn.ReLU(),\n","                             nn.Linear(hidden_size, hidden_size),\n","                             nn.ReLU(),\n","                             nn.Linear(hidden_size, action_size),\n","                             nn.Softmax(dim=1))\n","\n","  def forward(self, x):\n","    \"\"\"Get policy from state\n","\n","      Args:\n","          state (tensor): current state, size (batch x state_size)\n","\n","      Returns:\n","          action_dist (tensor): probability distribution over actions (batch x action_size)\n","    \"\"\"\n","    return self.net(x)\n","\n","\n","# Value Network\n","class ValueNetwork(nn.Module):\n","  def __init__(self, state_size):\n","    super().__init__()\n","    hidden_size = 8\n","\n","    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n","                             nn.ReLU(),\n","                             nn.Linear(hidden_size, hidden_size),\n","                             nn.ReLU(),\n","                             nn.Linear(hidden_size, hidden_size),\n","                             nn.ReLU(),\n","                             nn.Linear(hidden_size, 1))\n","\n","  def forward(self, x):\n","    \"\"\"Estimate value given state\n","\n","      Args:\n","          state (tensor): current state, size (batch x state_size)\n","\n","      Returns:\n","          value (tensor): estimated value, size (batch)\n","    \"\"\"\n","    return self.net(x)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### Main"],"metadata":{}},{"source":["\n","\n","def ppo_main():\n","  # Hyper parameters\n","  lr = 1e-3\n","  epochs = 20\n","  env_samples = 100\n","  gamma = 0.9\n","  batch_size = 256\n","  epsilon = 0.2\n","  policy_epochs = 5\n","\n","  # Init environment\n","  state_size = 4\n","  action_size = 2\n","  env = gym.make('CartPole-v1')\n","\n","  # Init networks\n","  policy_network = PolicyNetwork(state_size, action_size).cuda()\n","  value_network = ValueNetwork(state_size).cuda()\n","\n","  # Init optimizer\n","  optim = torch.optim.Adam(\n","      chain(policy_network.parameters(), value_network.parameters()), lr=lr)\n","\n","  # Start main loop\n","  results_ppo = []\n","  loop = tqdm(total=epochs, position=0, leave=False)\n","  for epoch in range(epochs):\n","\n","    memory = []  # Reset memory every epoch\n","    rewards = []  # Calculate average episodic reward per epoch\n","\n","    # Begin experience loop\n","    for episode in range(env_samples):\n","\n","      # Reset environment\n","      state = env.reset()\n","      done = False\n","      rollout = []\n","      cum_reward = 0  # Track cumulative reward\n","\n","      # Begin episode\n","      while not done and cum_reward < 200:  # End after 200 steps\n","        # Get action\n","        action, action_dist = get_action_ppo(policy_network, state)\n","\n","        # Take step\n","        next_state, reward, done, _ = env.step(action)\n","        # env.render()\n","\n","        # Store step\n","        rollout.append((state, action, action_dist, reward))\n","\n","        cum_reward += reward\n","        state = next_state  # Set current state\n","\n","      # Calculate returns and add episode to memory\n","      memory = calculate_return(memory, rollout, gamma)\n","\n","      rewards.append(cum_reward)\n","\n","    # Train\n","    dataset = RLDataset(memory)\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    learn_ppo(optim, policy_network, value_network,\n","              loader, epsilon, policy_epochs)\n","\n","    # Print results\n","    results_ppo.extend(rewards)  # Store rewards for this epoch\n","    loop.update(1)\n","    loop.set_description(\n","        \"Epochs: {} Reward: {}\".format(epoch, results_ppo[-1]))\n","\n","  return results_ppo\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["results_ppo = ppo_main()\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plt.plot(results_ppo)\n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}