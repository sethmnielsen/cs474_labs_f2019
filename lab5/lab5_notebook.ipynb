{"cells":[{"cell_type":"markdown","metadata":{},"source":["  <a href=\"https://colab.research.google.com/github/sethmnielsen/cs474_labs_f2019/blob/master/DL_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{},"source":["  # Lab 5: Style Transfer\n","\n","  ## Objective\n","  To explore an alternative use of DNNs by implementing the style transfer algorithm.\n","  To understand the importance of a complex loss function.\n","  To see how we can optimize not only over network parameters,\n","  but over other objects (such as images) as well.\n","\n","  ## Deliverable\n","  For this lab, you will need to implement the style transfer algorithm of Gatys et al.\n","\n","  * You must extract statistics from the content and style images\n","  * You must formulate an optimization problem over an input image\n","  * You must optimize the image to match both style and content\n","\n","  In your jupyter notebook, you should turn in the following:\n","\n","  * The final image that you generated\n","  * Your code\n","\n","  An example image that I generated is shown below\n","\n","  ![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=300&tok=179805&media=cs501r_f2016:style1.png)\n","\n","  ## Grading standards\n","  Your code will be graded on the following:\n","\n","  * 35% Correct extraction of statistics\n","  * 45% Correct construction of loss function in a loss class\n","  * 10% Correct initialization and optimization of image variable in a dataset class\n","  * 10% Awesome looking final image\n","\n","  ## Description:\n","\n","  For this lab, you should implement the style transfer algorithm referenced above.\n","  To do this, you will need to unpack the given images.\n","  Since we want you to focus on implementing the paper and the loss function,\n","  we will give you the code for this."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"import torchvision.models as models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import Tensor\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms, utils, datasets\nfrom tqdm import tqdm\nfrom torch.nn.parameter import Parameter\nimport pdb\nimport torchvision\nimport os\nimport gzip\nimport tarfile\nimport gc\nfrom PIL import Image\nimport io\nfrom IPython.core.ultratb import AutoFormattedTB\n__ITB__ = AutoFormattedTB(mode='Verbose', color_scheme='LightBg', tb_offset=1)\n\n"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"\nload_and_normalize = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\ndef upload():\n  print('Upload Content Image')\n  file_dict = files.upload()\n  content_path = io.BytesIO(file_dict[next(iter(file_dict))])\n\n  print('\\nUpload Style Image')\n  file_dict = files.upload()\n  style_path = io.BytesIO(file_dict[next(iter(file_dict))])\n  return content_path, style_path\n\ncontent_path, style_path = upload()\n# content_path = \"/home/seth/Pictures/style_transfer/iss043e93251_lrg.jpg\"\n# style_path = \"/home/seth/Pictures/style_transfer/modernism.jpg\"\nprint(f\"Content Path: {content_path}\")\nprint(f\"Style Path: {style_path}\")\n\n"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"# After the images are uploaded on to the local filesystem, you can use:\ncontent_image_orig = Image.open(content_path)\ncontent_image = load_and_normalize(\n    np.array(content_image_orig)).unsqueeze(0).cuda()\nstyle_image_orig = Image.open(style_path)\nstyle_image = load_and_normalize(\n    np.array(style_image_orig)).unsqueeze(0).cuda()\n"},{"cell_type":"markdown","metadata":{},"source":["  ___\n","\n","  ### Part 1\n","  Create a class to extract the layers needed for statistics\n","\n","  **TODO:**\n","\n","  * Use the pretrained VGG in your model\n","  * Gather statistics from the outputs of intermediate layers for the content image\n","  * Gather statistics for the style image\n","\n","  **DONE:**\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"\n\nclass Normalization(nn.Module):\n    def __init__(self, mean=torch.tensor([0.485, 0.456, 0.406]).cuda(), std=torch.tensor([0.229, 0.224, 0.225]).cuda()):\n        super(Normalization, self).__init__()\n        self.mean = torch.tensor(mean).view(-1, 1, 1)\n        self.std = torch.tensor(std).view(-1, 1, 1)\n\n    def forward(self, img):\n        return (img - self.mean) / self.std\n\n\nclass VGGIntermediate(nn.Module):\n    def __init__(self, requested=[]):\n        super(VGGIntermediate, self).__init__()\n        self.norm = Normalization().eval()\n        self.intermediates = {}\n        self.vgg = models.vgg16(pretrained=True).features.eval()\n        for i, m in enumerate(self.vgg.children()):\n            # we want to set the relu layers to NOT do the relu in place.\n            if isinstance(m, nn.ReLU):\n                # the model has a hard time going backwards on the in place functions.\n                m.inplace = False\n\n            if i in requested:\n                def curry(i):\n                    def hook(module, input, output):\n                        self.intermediates[i] = output\n                    return hook\n                m.register_forward_hook(curry(i))\n\n    def forward(self, x):\n        self.vgg(self.norm(x))\n        return self.intermediates\n\n"},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"vgg_names = [\"conv1_1\", \"relu1_1\", \"conv1_2\", \"relu1_2\", \"maxpool1\", \"conv2_1\", \"relu2_1\", \"conv2_2\", \"relu2_2\", \"maxpool2\", \"conv3_1\", \"relu3_1\", \"conv3_2\", \"relu3_2\", \"conv3_3\",\n             \"relu3_3\", \"maxpool3\", \"conv4_1\", \"relu4_1\", \"conv4_2\", \"relu4_2\", \"conv4_3\", \"relu4_3\", \"maxpool4\", \"conv5_1\", \"relu5_1\", \"conv5_2\", \"relu5_2\", \"conv5_3\", \"relu5_3\", \"maxpool5\"]\n\n# Choose the layers to use for style and content transfer\ncontent_inds = [vgg_names.index(\"conv4_2\")]\nstyle_inds = [vgg_names.index(\"conv1_1\"), vgg_names.index(\"conv2_1\"), vgg_names.index(\n    \"conv3_1\"), vgg_names.index(\"conv4_1\"), vgg_names.index(\"conv5_1\")]\n\n# Create the vgg network in eval mode\n#  with our forward method that returns the outputs of the intermediate layers we requested\nstyle_vgg = VGGIntermediate(style_inds)\ncontent_vgg = VGGIntermediate(content_inds)\n\n# Cache the outputs of the content and style layers for their respective images\nstyle_layers = style_vgg(style_image)\ncontent_layers = content_vgg(content_image)\n\n"},{"cell_type":"markdown","metadata":{},"source":[" ___\n","\n","  ### Part 2\n","  Create a method to turn a tensor to an image to display\n","\n","  **TODO:**\n","  * Display the style tensor and content tensor transformed back to an image\n","\n","  **DONE:**\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"toPIL = transforms.ToPILImage()\n\n\ndef display(tensor, title=None):\n    image = tensor.cpu().clone()\n    image = image.squeeze(0)\n    image = toPIL(image)\n    plt.imshow(image)\n    if title is not None:\n        plt.title(title)\n\n\nplt.figure()\ndisplay(style_image, title='Style Image')\n\nplt.figure()\ndisplay(content_image, title='Content Image')\n"},{"cell_type":"markdown","metadata":{},"source":[" ___\n","\n","  ### Part 3\n","  Create classes for the style and content loss\n","\n","  **TODO:**\n","\n","  * Create a module that calculates the content loss in the forward method, compared to some precalculated targets stored in the class\n","  * Create a module that calculates the style loss in the forward method using a gram matrix, compared to some precalculated targets stored in the class\n","\n","  **DONE:**\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"def gram_matrix(activation: Tensor) -> Tensor:\n    b, c, h, w = activation.shape # batch, channels, height, width\n    features = activation.view(b*c, h*w)\n    basis = features @ features.t()  # orthonormal basis\n    gram = basis / (b*c*h*w)  # normalizing\n    return gram\n\n\nclass ContentLoss(nn.Module):\n    \n    def __init__(self, inds, img):\n        super(ContentLoss, self).__init__()\n        self.vgg = VGGIntermediate(inds).cuda()\n        activations: dict = self.vgg(img)\n        self.P = []\n        for key in activations.keys():\n            Fl = activations[key].detach()\n            self.P.append(Fl)\n\n    def forward(self, x):\n        activations: dict = self.vgg(x)\n        loss = 0\n        for l, key in enumerate(activations.keys()):\n            Fl = activations[key]\n            loss += torch.sum( (Fl - self.P[l])**2 ) / Fl.numel()\n        loss *= 0.5\n        return loss\n\n\nclass StyleLoss(nn.Module):\n\n    def __init__(self, inds, img):\n        super(StyleLoss, self).__init__()\n        self.vgg = VGGIntermediate(inds).cuda()\n        activations: dict = self.vgg(img)\n        self.gees = []\n        for key in activations.keys():\n            Fl = activations[key].detach()\n            self.gees.append(gram_matrix(Fl))\n\n    def forward(self, x):\n        activations: dict = self.vgg(x)\n        loss = 0\n        for l, key in enumerate(activations.keys()):\n            G = gram_matrix(activations[key])\n            loss += F.mse_loss( G, self.grams[l] ) / len(activations)\n        return loss\n\n# Instantiate a content loss module for each content layer\n#  with the content reference image outputs for that layer for comparison\ncontent_obj = ContentLoss(content_inds, content_image)\n\n# Instantiate a sytle loss module for each style layer\n#  with the style reference image outputs for that layer for comparison\nstyle_obj = StyleLoss(style_inds, style_image)\n"},{"cell_type":"markdown","metadata":{},"source":["  ___\n","\n","  ### Part 4\n","  Create and run a method that minimizes the content and style loss for a copy of the content image\n","\n","  Note that the content loss should be zero if you take out the style loss. Why is that?\n","\n","  **TODO:**\n","\n","  * Use an Adam optimizer with learning rate of .1\n","  * Show both the content and the style loss every 50 steps\n","  * Ensure that the outputs don't go out of range (clamp them)\n","  * Display the tensor as an image!\n","\n","  **DONE:**\n","\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"# Start with a copy of the content image\nimg = content_image.clone()\n\n# Run the optimizer on the images to change the image\n#  using the loss of the style and content layers\n#  to backpropagate errors\noptimizer = optim.Adam([img.requires_grad_()], lr=0.1)\n\nlosses = []\nalpha = 1.0\nbeta = 10000.0\nnum_epochs = 500\nloop_rate = 50\nloop = tqdm(total=num_epochs//loop_rate+1, position=0, leave=False)\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n\n    content_loss = content_obj(img) * alpha \n    style_loss = style_obj(img) * beta\n    loss = style_loss + content_loss\n    if epoch % loop_rate == 0:\n        losses.append(loss.item())\n        loop.set_description(f'epoch:{epoch}, \\\n                               style_loss:{style_loss.item()}, \\\n                               content_loss:{content_loss.item()}')\n        loop.update(1)\n\n    loss.backward()\n    optimizer.step()\n\n    img.data.clamp_(0,1)\n\nloop.close()\n\n# Show the image\nplt.figure()\ndisplay(img, title='Stylized Image')\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}